@misc{vaswani2017attention,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2017},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{garnelo2018neural,
  title         = {Neural Processes},
  author        = {Marta Garnelo and Jonathan Schwarz and Dan Rosenbaum and Fabio Viola and Danilo J. Rezende and S. M. Ali Eslami and Yee Whye Teh},
  year          = {2018},
  eprint        = {1807.01622},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{garnelo2018conditional,
  title         = {Conditional Neural Processes},
  author        = {Marta Garnelo and Dan Rosenbaum and Chris J. Maddison and Tiago Ramalho and David Saxton and Murray Shanahan and Yee Whye Teh and Danilo J. Rezende and S. M. Ali Eslami},
  year          = {2018},
  eprint        = {1807.01613},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{shaw2018selfattention,
  title         = {Self-Attention with Relative Position Representations},
  author        = {Peter Shaw and Jakob Uszkoreit and Ashish Vaswani},
  year          = {2018},
  eprint        = {1803.02155},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{wu2021rethinking,
  title         = {Rethinking and Improving Relative Position Encoding for Vision Transformer},
  author        = {Kan Wu and Houwen Peng and Minghao Chen and Jianlong Fu and Hongyang Chao},
  year          = {2021},
  eprint        = {2107.14222},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{kazemnejad2019:pencoding,
  title   = {Transformer Architecture: The Positional Encoding},
  author  = {Kazemnejad, Amirhossein},
  journal = {kazemnejad.com},
  year    = {2019},
  url     = {https://kazemnejad.com/blog/transformer_architecture_positional_encoding/}
}
@article{weng2018attention,
  title   = {Attention? Attention!},
  author  = {Weng, Lilian},
  journal = {lilianweng.github.io},
  year    = {2018},
  url     = {https://lilianweng.github.io/posts/2018-06-24-attention/}
}
@misc{nguyen2023transformer,
  title         = {Transformer Neural Processes: Uncertainty-Aware Meta Learning Via Sequence Modeling},
  author        = {Tung Nguyen and Aditya Grover},
  year          = {2023},
  eprint        = {2207.04179},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{kim2019attentive,
  title         = {Attentive Neural Processes},
  author        = {Hyunjik Kim and Andriy Mnih and Jonathan Schwarz and Marta Garnelo and Ali Eslami and Dan Rosenbaum and Oriol Vinyals and Yee Whye Teh},
  year          = {2019},
  eprint        = {1901.05761},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{gordon2020convolutional,
  title         = {Convolutional Conditional Neural Processes},
  author        = {Jonathan Gordon and Wessel P. Bruinsma and Andrew Y. K. Foong and James Requeima and Yann Dubois and Richard E. Turner},
  year          = {2020},
  eprint        = {1910.13556},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}