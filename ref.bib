@misc{vaswani2017attention,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2017},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{garnelo2018neural,
  title         = {Neural Processes},
  author        = {Marta Garnelo and Jonathan Schwarz and Dan Rosenbaum and Fabio Viola and Danilo J. Rezende and S. M. Ali Eslami and Yee Whye Teh},
  year          = {2018},
  eprint        = {1807.01622},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{garnelo2018conditional,
  title         = {Conditional Neural Processes},
  author        = {Marta Garnelo and Dan Rosenbaum and Chris J. Maddison and Tiago Ramalho and David Saxton and Murray Shanahan and Yee Whye Teh and Danilo J. Rezende and S. M. Ali Eslami},
  year          = {2018},
  eprint        = {1807.01613},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{shaw2018selfattention,
  title         = {Self-Attention with Relative Position Representations},
  author        = {Peter Shaw and Jakob Uszkoreit and Ashish Vaswani},
  year          = {2018},
  eprint        = {1803.02155},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{wu2021rethinking,
  title         = {Rethinking and Improving Relative Position Encoding for Vision Transformer},
  author        = {Kan Wu and Houwen Peng and Minghao Chen and Jianlong Fu and Hongyang Chao},
  year          = {2021},
  eprint        = {2107.14222},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{kazemnejad2019:pencoding,
  title   = {Transformer Architecture: The Positional Encoding},
  author  = {Kazemnejad, Amirhossein},
  journal = {kazemnejad.com},
  year    = {2019},
  url     = {https://kazemnejad.com/blog/transformer_architecture_positional_encoding/}
}
@article{weng2018attention,
  title   = {Attention? Attention!},
  author  = {Weng, Lilian},
  journal = {lilianweng.github.io},
  year    = {2018},
  url     = {https://lilianweng.github.io/posts/2018-06-24-attention/}
}
@misc{nguyen2023transformer,
  title         = {Transformer Neural Processes: Uncertainty-Aware Meta Learning Via Sequence Modeling},
  author        = {Tung Nguyen and Aditya Grover},
  year          = {2023},
  eprint        = {2207.04179},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{kim2019attentive,
  title         = {Attentive Neural Processes},
  author        = {Hyunjik Kim and Andriy Mnih and Jonathan Schwarz and Marta Garnelo and Ali Eslami and Dan Rosenbaum and Oriol Vinyals and Yee Whye Teh},
  year          = {2019},
  eprint        = {1901.05761},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{gordon2020convolutional,
  title         = {Convolutional Conditional Neural Processes},
  author        = {Jonathan Gordon and Wessel P. Bruinsma and Andrew Y. K. Foong and James Requeima and Yann Dubois and Richard E. Turner},
  year          = {2020},
  eprint        = {1910.13556},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}@book{books/lib/RasmussenW06,
  added-at  = {2020-07-17T00:00:00.000+0200},
  author    = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  biburl    = {https://www.bibsonomy.org/bibtex/2670a576a21065048f7ddede17e09b6b4/dblp},
  ee        = {https://www.worldcat.org/oclc/61285753},
  interhash = {72c030472023000e0bdeeb06081c3764},
  intrahash = {670a576a21065048f7ddede17e09b6b4},
  isbn      = {026218253X},
  keywords  = {dblp},
  pages     = {I-XVIII, 1-248},
  publisher = {MIT Press},
  series    = {Adaptive computation and machine learning},
  timestamp = {2020-07-24T00:45:17.000+0200},
  title     = {Gaussian processes for machine learning.},
  year      = 2006
}
@misc{zaheer2018deep,
  title         = {Deep Sets},
  author        = {Manzil Zaheer and Satwik Kottur and Siamak Ravanbakhsh and Barnabas Poczos and Ruslan Salakhutdinov and Alexander Smola},
  year          = {2018},
  eprint        = {1703.06114},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}@misc{kim2019attentive,
  title         = {Attentive Neural Processes},
  author        = {Hyunjik Kim and Andriy Mnih and Jonathan Schwarz and Marta Garnelo and Ali Eslami and Dan Rosenbaum and Oriol Vinyals and Yee Whye Teh},
  year          = {2019},
  eprint        = {1901.05761},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{dosovitskiy2021image,
  title         = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author        = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  year          = {2021},
  eprint        = {2010.11929},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{devlin2019bert,
  title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author        = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  year          = {2019},
  eprint        = {1810.04805},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{brown2020language,
  title         = {Language Models are Few-Shot Learners},
  author        = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  year          = {2020},
  eprint        = {2005.14165},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}