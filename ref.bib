@misc{vaswani2017attention,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2017},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{garnelo2018neural,
  title         = {Neural Processes},
  author        = {Marta Garnelo and Jonathan Schwarz and Dan Rosenbaum and Fabio Viola and Danilo J. Rezende and S. M. Ali Eslami and Yee Whye Teh},
  year          = {2018},
  eprint        = {1807.01622},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{shaw2018selfattention,
  title         = {Self-Attention with Relative Position Representations},
  author        = {Peter Shaw and Jakob Uszkoreit and Ashish Vaswani},
  year          = {2018},
  eprint        = {1803.02155},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{wu2021rethinking,
  title         = {Rethinking and Improving Relative Position Encoding for Vision Transformer},
  author        = {Kan Wu and Houwen Peng and Minghao Chen and Jianlong Fu and Hongyang Chao},
  year          = {2021},
  eprint        = {2107.14222},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{kazemnejad2019:pencoding,
  title   = {Transformer Architecture: The Positional Encoding},
  author  = {Kazemnejad, Amirhossein},
  journal = {kazemnejad.com},
  year    = {2019},
  url     = {https://kazemnejad.com/blog/transformer_architecture_positional_encoding/}
}
@article{weng2018attention,
  title   = {Attention? Attention!},
  author  = {Weng, Lilian},
  journal = {lilianweng.github.io},
  year    = {2018},
  url     = {https://lilianweng.github.io/posts/2018-06-24-attention/}
}
