@misc{vaswani2017attention,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2017},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{garnelo2018neural,
  title         = {Neural Processes},
  author        = {Marta Garnelo and Jonathan Schwarz and Dan Rosenbaum and Fabio Viola and Danilo J. Rezende and S. M. Ali Eslami and Yee Whye Teh},
  year          = {2018},
  eprint        = {1807.01622},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{garnelo2018conditional,
  title         = {Conditional Neural Processes},
  author        = {Marta Garnelo and Dan Rosenbaum and Chris J. Maddison and Tiago Ramalho and David Saxton and Murray Shanahan and Yee Whye Teh and Danilo J. Rezende and S. M. Ali Eslami},
  year          = {2018},
  eprint        = {1807.01613},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{shaw2018selfattention,
  title         = {Self-Attention with Relative Position Representations},
  author        = {Peter Shaw and Jakob Uszkoreit and Ashish Vaswani},
  year          = {2018},
  eprint        = {1803.02155},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{wu2021rethinking,
  title         = {Rethinking and Improving Relative Position Encoding for Vision Transformer},
  author        = {Kan Wu and Houwen Peng and Minghao Chen and Jianlong Fu and Hongyang Chao},
  year          = {2021},
  eprint        = {2107.14222},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@article{kazemnejad2019:pencoding,
  title   = {Transformer Architecture: The Positional Encoding},
  author  = {Kazemnejad, Amirhossein},
  journal = {kazemnejad.com},
  year    = {2019},
  url     = {https://kazemnejad.com/blog/transformer_architecture_positional_encoding/}
}
@article{weng2018attention,
  title   = {Attention? Attention!},
  author  = {Weng, Lilian},
  journal = {lilianweng.github.io},
  year    = {2018},
  url     = {https://lilianweng.github.io/posts/2018-06-24-attention/}
}
@misc{nguyen2023transformer,
  title         = {Transformer Neural Processes: Uncertainty-Aware Meta Learning Via Sequence Modeling},
  author        = {Tung Nguyen and Aditya Grover},
  year          = {2023},
  eprint        = {2207.04179},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{kim2019attentive,
  title         = {Attentive Neural Processes},
  author        = {Hyunjik Kim and Andriy Mnih and Jonathan Schwarz and Marta Garnelo and Ali Eslami and Dan Rosenbaum and Oriol Vinyals and Yee Whye Teh},
  year          = {2019},
  eprint        = {1901.05761},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{gordon2020convolutional,
  title         = {Convolutional Conditional Neural Processes},
  author        = {Jonathan Gordon and Wessel P. Bruinsma and Andrew Y. K. Foong and James Requeima and Yann Dubois and Richard E. Turner},
  year          = {2020},
  eprint        = {1910.13556},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}@book{books/lib/RasmussenW06,
  added-at  = {2020-07-17T00:00:00.000+0200},
  author    = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  biburl    = {https://www.bibsonomy.org/bibtex/2670a576a21065048f7ddede17e09b6b4/dblp},
  ee        = {https://www.worldcat.org/oclc/61285753},
  interhash = {72c030472023000e0bdeeb06081c3764},
  intrahash = {670a576a21065048f7ddede17e09b6b4},
  isbn      = {026218253X},
  keywords  = {dblp},
  pages     = {I-XVIII, 1-248},
  publisher = {MIT Press},
  series    = {Adaptive computation and machine learning},
  timestamp = {2020-07-24T00:45:17.000+0200},
  title     = {Gaussian processes for machine learning.},
  year      = 2006
}
@misc{zaheer2018deep,
  title         = {Deep Sets},
  author        = {Manzil Zaheer and Satwik Kottur and Siamak Ravanbakhsh and Barnabas Poczos and Ruslan Salakhutdinov and Alexander Smola},
  year          = {2018},
  eprint        = {1703.06114},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}@misc{kim2019attentive,
  title         = {Attentive Neural Processes},
  author        = {Hyunjik Kim and Andriy Mnih and Jonathan Schwarz and Marta Garnelo and Ali Eslami and Dan Rosenbaum and Oriol Vinyals and Yee Whye Teh},
  year          = {2019},
  eprint        = {1901.05761},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{dosovitskiy2021image,
  title         = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author        = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  year          = {2021},
  eprint        = {2010.11929},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{devlin2019bert,
  title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author        = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  year          = {2019},
  eprint        = {1810.04805},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{brown2020language,
  title         = {Language Models are Few-Shot Learners},
  author        = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  year          = {2020},
  eprint        = {2005.14165},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}@inproceedings{feng2022efficient,
  title     = {Efficient Queries Transformer Neural Processes},
  author    = {Leo Feng and Hossein Hajimirsadeghi and Yoshua Bengio and Mohamed Osama Ahmed},
  booktitle = {Sixth Workshop on Meta-Learning at the Conference on Neural Information Processing Systems},
  year      = {2022},
  url       = {https://openreview.net/forum?id=_3FyT_W1DW}
}
@inproceedings{anonymous2024translationequivariant,
  title     = {Translation-Equivariant Transformer Neural Processes},
  author    = {Ashman, Matthew and Diaconu, Cristiana and Kim, Junhyuck and Sivaraya, Lakee and Markou, Stratis and Requeima, James and Bruinsma, Wessel P. and Turner, Richard E.},
  booktitle = {Forty-first International Conference on Machine Learning},
  year      = {2024},
  url       = {https://openreview.net/forum?id=pftXzp6Yn3}
}@misc{jaegle2021perceiver,
  title         = {Perceiver: General Perception with Iterative Attention},
  author        = {Andrew Jaegle and Felix Gimeno and Andrew Brock and Andrew Zisserman and Oriol Vinyals and Joao Carreira},
  year          = {2021},
  eprint        = {2103.03206},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{lee2019set,
  title         = {Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks},
  author        = {Juho Lee and Yoonho Lee and Jungtaek Kim and Adam R. Kosiorek and Seungjin Choi and Yee Whye Teh},
  year          = {2019},
  eprint        = {1810.00825},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{feng2023latent,
  title         = {Latent Bottlenecked Attentive Neural Processes},
  author        = {Leo Feng and Hossein Hajimirsadeghi and Yoshua Bengio and Mohamed Osama Ahmed},
  year          = {2023},
  eprint        = {2211.08458},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{katharopoulos2020transformers,
  title         = {Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},
  author        = {Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and François Fleuret},
  year          = {2020},
  eprint        = {2006.16236},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{clevert2016fast,
  title         = {Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)},
  author        = {Djork-Arné Clevert and Thomas Unterthiner and Sepp Hochreiter},
  year          = {2016},
  eprint        = {1511.07289},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}@misc{tolstikhin2021mlpmixer,
  title         = {MLP-Mixer: An all-MLP Architecture for Vision},
  author        = {Ilya Tolstikhin and Neil Houlsby and Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Thomas Unterthiner and Jessica Yung and Andreas Steiner and Daniel Keysers and Jakob Uszkoreit and Mario Lucic and Alexey Dosovitskiy},
  year          = {2021},
  eprint        = {2105.01601},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{mai2023hypermixer,
  title         = {HyperMixer: An MLP-based Low Cost Alternative to Transformers},
  author        = {Florian Mai and Arnaud Pannatier and Fabio Fehr and Haolin Chen and Francois Marelli and Francois Fleuret and James Henderson},
  year          = {2023},
  eprint        = {2203.03691},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{ha2016hypernetworks,
  title         = {HyperNetworks},
  author        = {David Ha and Andrew Dai and Quoc V. Le},
  year          = {2016},
  eprint        = {1609.09106},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}@misc{hensman2013gaussian,
  title         = {Gaussian Processes for Big Data},
  author        = {James Hensman and Nicolo Fusi and Neil D. Lawrence},
  year          = {2013},
  eprint        = {1309.6835},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}@misc{hendrycks2023gaussian,
  title         = {Gaussian Error Linear Units (GELUs)},
  author        = {Dan Hendrycks and Kevin Gimpel},
  year          = {2023},
  eprint        = {1606.08415},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}@misc{ronneberger2015unet,
  title         = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  author        = {Olaf Ronneberger and Philipp Fischer and Thomas Brox},
  year          = {2015},
  eprint        = {1505.04597},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{he2015deep,
  title         = {Deep Residual Learning for Image Recognition},
  author        = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  year          = {2015},
  eprint        = {1512.03385},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}@misc{simonyan2015deep,
  title         = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author        = {Karen Simonyan and Andrew Zisserman},
  year          = {2015},
  eprint        = {1409.1556},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@inproceedings{NIPS2012_c399862d,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
  volume    = {25},
  year      = {2012}
}
