\documentclass[../../main.tex]{subfiles}

\begin{document}

\section{Transformer Neural Processes}

\subsection{Introduction}

Transformer Neural Processes (TNPs) \cite{nguyen2023transformer} were introduced to overcome the limitations of the Latent Neural Process (LNP) \cite{garnelo2018neural} which have the following limitations:

\begin{itemize}
	\item LNPs have intracatable likelihoods hence they have to be trained using variational inference by optimizing for the ELBO. It has been shown that the ELBO can be a poor approximation of the true likelihood hence the model can be poorly trained.
	\item LNPs tend to underfit the data and are unable to capture the complexity of the data.
\end{itemize}

TNPs overcome these limitations by using the Transformer \cite{vaswani2017attention} to utilize attention to learn the relationships between the context points. This allows the model to capture the complexity of the data and also allows the model to be trained using maximum likelihood estimation (MLE) instead of variational inference as the likelihood is tractable.

Attention has already been implemented before \cite{nguyen2023transformer} in \cite{kim2019attentive} however according to \cite{nguyen2023transformer} the attention mechanism used in \cite{kim2019attentive} tends to make overconfident predictions and have poor performance on sequential decision-making problems.

\subsection{Requirements}

As NPs are a meta-learning method they should be trained on a context set which is a subset of a larger dataset. In this example, say we sample $N$ $x-y$ pairs from a process $\mathcal{F}$, we designate the first $N_c$ pairs as the context set and the remaining $N-N_c$ pairs as the target set, or mathematically $\mathcal{C} = \{(x_i, y_i)\}_{i=1}^{N_c}$ and $\mathcal{T} = \{(x_i, y_i)\}_{i=N_c+1}^{N}$. The objective function is the maximum likelihood estimation of the target set given the context set autoregressively, or mathematically on the target set.


\begin{equation}
	\mathcal{L}(\theta) = \mathbb{E}_{(x, y) \sim \mathcal{F}} \left[ \log p(y_{N_c+1:N} | x_{N_c+1:N}, \mathcal{C}; \theta) \right]
	\end{equation}

Where $\theta$ are the parameters of the model and $y_{N_c+1:N}$ and $x_{N_c+1:N}$ are the target set. Since we pass the target set through the model autoregressively, the objective function can be rewritten as:

\begin{equation}
	\mathcal{L}(\theta) = \mathbb{E}_{(x, y) \sim \mathcal{F}} \left[ \sum_{i=N_c+1}^{N} \log p(y_i | x_i, \mathcal{C}, x_{N_c+1:i-1}, y_{N_c+1:i-1}; \theta) \right]
\end{equation}

Where each conditional is modeled as a Gaussian distribution with mean and variance predicted by the model. 

There are two important properties of TNPs that we need to build upon:

\begin{itemize}
	\item \textbf{Context Invariance}: If we permute the context set, the model should not change its predictions
	 \item \textbf{Target Equivariance} If we permute the target set, the model should not change its predictions, e.g. we shift the target set by one point to the left, and the model's predictions should also shift by one point to the left.
\end{itemize}

\subsection{Autoregressive Transformer Neural Processes (TNP-A)}

We can not implement the traditional Transformer as the inclusion of the positional encoding breaks the context invariance property since the positional encoding is dependent on the position of the points, but we need to allow the model to learn the relationships between $x$ and $y$ \textbf{pairs}. To do this, \cite{nguyen2023transformer} concatenates the $x$ and $y$ pairs together into a single vector for the context set and target set. They then add auxiliary tokens from the target set but with $y=0$ such that the dataset looks like

\[
	 {(x_1, y_1), . . . ,(x_N , y_N ),(x_{N_c+1}, 0), . . . ,(x_N , 0)}
\]

As we can see the $x$ values from the target set effectively appear twice, the auxiliary tokens are used to represent the target set and then the corresponding $y$ values are used to train the model since those tokens are not auxiliary. To make this work, we must employ a masking scheme to allow:

\bul{
	\item Context tokens to attend to themselves
	\item Target tokens to attend to context and previous target tokens
	\item Auxillary tokens to attend to context and previous target tokens
}

This model is referred to as the Autoregressive Transformer Neural Process (TNP-A) in \cite{nguyen2023transformer}.

\ifSubfilesClassLoaded{%
    \printbibliography{}
}{} 


\end{document}