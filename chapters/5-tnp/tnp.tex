\documentclass[../../main.tex]{subfiles}

\begin{document}

\section{Transformer Neural Processes}

\subsection{Vanilla Transformer Neural Process}

An Attention based encoder for the Neural Process was investigated in \cite{kim2019attentive}, though the results were impressive, the model fails to perform at larger scale and `tends to make overconfident predictions and have poor performance on sequential decision-making problems' \cite{nguyen2023transformer}. It is natural to think that the Transformer \cite{vaswani2017attention} could be used to improve the performance of the Neural Process, as Transformers have been proven to effectively model large scale data using attention mechanisms. \cite{nguyen2023transformer} introduced the Transformer Neural Process (TNP) which uses an \emph{encoder-only} Transformer to learn the relationships between the context and target points via self-attention with appropriate masking. 

\subsubsection{Model Architecture}

Similar to the standard Neural Process architecture we are required to encode datapoints within the context set into a vector representation / tokens. Where the TNP differs is that it also encodes the target points along which is padded with zeros to represent the lack of value for the target data points. We introduce a flag bit to indicate whether the token is a context or target token. Consider the context dataset $\mathcal{C} = \{(\bm{x_i}, \bm{y_i})\}_{i=1}^{N_c}$ and target set $\mathcal{T} = \{(\bm{x_i})\}_{i=N_c+1}^{N}$, the model will encode a datapoint into a token $\bm{t_i}$ as follows:

\[
	\bm{t_i} = \begin{cases}
		\text{MLP}(\texttt{cat}[\bm{x_i},  0, \bm{y_i}]) & \text{if } i \leq N_c \\
		\text{MLP}(\texttt{cat}[\bm{x_i},  1, \bm{ 0}]) & \text{if } i > N_c
	\end{cases}
\]

Where we use a flag bit of 0 to indicate a context token and 1 to indicate a target token. Now that the datapoints are tokenized we can pass them through the Transformer encoder to learn the relationships between the context and target points. Importantly the Transformer encoder is masked such that the target tokens can only attend to the context tokens and previous target tokens

\todo{ADD FIGURE OF MASKING}

\subsection{Requirements}

As NPs are a meta-learning method they should be trained on a context set which is a subset of a larger dataset. In this example, say we sample $N$ $x-y$ pairs from a process $\mathcal{F}$, we designate the first $N_c$ pairs as the context set and the remaining $N-N_c$ pairs as the target set, or mathematically $\mathcal{C} = \{(x_i, y_i)\}_{i=1}^{N_c}$ and $\mathcal{T} = \{(x_i, y_i)\}_{i=N_c+1}^{N}$. The objective function is the maximum likelihood estimation of the target set given the context set autoregressively, or mathematically on the target set.


\begin{equation}
	\mathcal{L}(\theta) = \mathbb{E}_{(x, y) \sim \mathcal{F}} \left[ \log p(y_{N_c+1:N} | x_{N_c+1:N}, \mathcal{C}; \theta) \right]
	\end{equation}

Where $\theta$ are the parameters of the model and $y_{N_c+1:N}$ and $x_{N_c+1:N}$ are the target set. Since we pass the target set through the model autoregressively, the objective function can be rewritten as:

\begin{equation}
	\mathcal{L}(\theta) = \mathbb{E}_{(x, y) \sim \mathcal{F}} \left[ \sum_{i=N_c+1}^{N} \log p(y_i | x_i, \mathcal{C}, x_{N_c+1:i-1}, y_{N_c+1:i-1}; \theta) \right]
\end{equation}

Where each conditional is modeled as a Gaussian distribution with mean and variance predicted by the model. 

There are two important properties of TNPs that we need to build upon:

\begin{itemize}
	\item \textbf{Context Invariance}: If we permute the context set, the model should not change its predictions
	 \item \textbf{Target Equivariance} If we permute the target set, the model should not change its predictions, e.g. we shift the target set by one point to the left, and the model's predictions should also shift by one point to the left.
\end{itemize}

\subsection{Autoregressive Transformer Neural Processes (TNP-A)}

We can not implement the traditional Transformer as the inclusion of the positional encoding breaks the context invariance property since the positional encoding is dependent on the position of the points, but we need to allow the model to learn the relationships between $x$ and $y$ \textbf{pairs}. To do this, \cite{nguyen2023transformer} concatenates the $x$ and $y$ pairs together into a single vector for the context set and target set. They then add auxiliary tokens from the target set but with $y=0$ such that the dataset looks like

\[
	 {(x_1, y_1), . . . ,(x_N , y_N ),(x_{N_c+1}, 0), . . . ,(x_N , 0)}
\]

As we can see the $x$ values from the target set effectively appear twice, the auxiliary tokens are used to represent the target set and then the corresponding $y$ values are used to train the model since those tokens are not auxiliary. To make this work, we must employ a masking scheme to allow:

\bul{
	\item Context tokens to attend to themselves
	\item Target tokens to attend to context and previous target tokens
	\item Auxillary tokens to attend to context and previous target tokens
}

This model is referred to as the Autoregressive Transformer Neural Process (TNP-A) in \cite{nguyen2023transformer}.

\ifSubfilesClassLoaded{%
    \printbibliography{}
}{} 


\end{document}