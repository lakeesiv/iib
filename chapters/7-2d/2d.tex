\documentclass[../../main.tex]{subfiles}

\begin{document}

\section{Datasets}

\subsection{Gaussian Process}

The 2D Gaussian Process is the natural extension of the 1D Gaussian Process described in \autoref{sec:1d-gp-dataset} where we use the squared exponential kernel. We continue to use the same range of lengthscale across both input dimensions as the 1D Gaussian Process. 

The following plots show some samples from the GP dataset.

\todo{Add plots of GP dataset}


\subsection{Sawtooth}

The 2D Sawtooth dataset is the natural extension of the 1D Sawtooth dataset described in \autoref{sec:1d-sawtooth-dataset}. We continue to use the same period $T$ and noise $n$ across both input dimensions as the 1D Sawtooth dataset.

The following plots show some samples from the Sawtooth dataset.

\todo{Add plots of Sawtooth dataset}

\subsection{Restricted Sawtooth}

By accident when generating the 2D Sawtooth dataset, we ended up restricted the `direction of travel' of the sawtooth function to the line of $x_1 = x_2$ or $x_1 = -x_2$. This was not intentional but when training both models on this dataset, we found very interesting results. As the models only learn a subset of the `full sawtooth' function, we can see how well the models can generalize to samples from the full sawtooth function.

The following plots show some samples from the Restricted Sawtooth dataset.

\section{Post or Pre Relative Attention Function}

\todo{Highlight MLP}

In our original formulation of the TETNP (\autoref{sec:tetnp}) we pass the matrix of differences ($\bm{\Delta}$) between $x$ values through a function $F$ to apply non-linearities then add it to the dot product attention \autoref{eq:relative-attention}, whilst this performs well we can also consider applying this non-linearity after combining the dot product attention and the relative attention, this method is called the `Post Relative Attention Function'. 

\begin{align}
	\text{Attention}(\bm{Q}, \bm{K}, \bm{V}, \bm{X}) &= \text{softmax}\left(\bm{E} \right) \bm{V}
\end{align}
\begin{align}
	\text{Pre:} \quad \bm{E_{ij}} &= \bm{D_{ij}}+ \text{MLP}(\bm{\Delta_{ij}})\\
    \text{Post:} \quad \bm{E_{ij}} &= \text{MLP}(\texttt{cat}[ \bm{D_{ij}}, \bm{\Delta_{ij}}])
\end{align}
Where 
\begin{align}
     \bm{D_{ij}} = \bm{q_i} \cdot \bm{k_j}  /\sqrt{d_k} \quad \quad    \bm{\Delta_{ij}} = \bm{x_i} - \bm{x_j}
\end{align}


We will investigate the performance of the TETNP with the `Post Relative Attention Function' compared to the original `Pre Relative Attention Function'. We choose to use the Sawtooth dataset as it is more diffucult to learn than the Gaussian Process dataset and will show the differences between the two models more clearly. 


\todo{Graphs of vals loss between relative attn functions}

The results show that the TETNP with the `Post Relative Attention Function' outperforms the TETNP with the `Pre Relative Attention Function' by quite a large margin. Trivially this makes a lot of sense as the Post function can further refine the dot product attention through the MLP whilst in the `Pre' function the MLP is \emph{only} applied to the $\bm{delta}$ matrix. The computational complexity of these two functions are not too different as the MLP are small and applied to the same size matrices. 


\section{ConvNP vs TETNP}

We have discovered in the 1D section that the TETNP outperforms the vanilla TNP in all cases, hence for the 2D experiments we will only compare the ConvNP to the TETNP. When performing our experiments we will use models which are both 1 million parameters in size, to ensure a fair comparison.

\subsection{Gaussian Process}

As mentioned previously, the Gaussian Process dataset is not very difficult to learn and the ConvNP and TETNP both perform very well on this dataset. 

\todo{Graph of VAL loss of TETNP vs ConvNP}
\todo{Plot of samples from TETNP and ConvNP}

\subsection{Sawtooth}

\subsubsection{Full Sawtooth}

\todo{Values for VAL loss of TETNP vs ConvNP}
\todo{Examples of samples from TETNP and ConvNP}

\subsubsection{Restricted Sawtooth}

\todo{Values for VAL loss of TETNP vs ConvNP}
\todo{Examples of samples from TETNP and ConvNP}

\subsection{Rotational Equivariance}

\todo{Plots from RE experiments and values}

\section{Computational Complexity}

\todo{BIG TODO}

\ifSubfilesClassLoaded{%
    \printbibliography{}
}{} 


\end{document}