\documentclass[main.tex]{subfiles}

\begin{document}


\section{Neural Processes}

Neural Processes are an expansion of Gaussian Processes using Neural Networks introduced in \cite{garnelo2018neural} they are trained on a context dataset which is. Neural processes are a meta-learning algorithm that can be used for few-shot learning. They are trained on a context dataset which is a set of input-output pairs $\mathcal{D} = \{ (x_i, y_i) \}_{i=1}^N$. 

\subsection{Meta Learning}

Meta-Learning is a method of ML where the model is trained on a set of tasks and then tested on a new task. The goal is to learn a model that can learn new tasks quickly`', i.e. `learning to learn'. These come under the terminology of few-shot learning where the model is trained on a small number of examples (more specifically, we can say $N$-way-$K$-shot learning where $N$ is the number of classes and $K$ is the number of examples per class).

Consider a set of datasets $\mathcal{D} = \{ \mathcal{D}_i \}_{i=1}^D$ where each dataset $\mathcal{D}_i$ is a set of input-output pairs $\mathcal{D}_i = \{ (x_{ij}, y_{ij}) \}_{j=1}^{N_i}$. The goal of meta-learning is to learn a model $f$ that can learn a new task $\mathcal{D}_{N+1}$ with a small number of examples, i.e. few shot learning.

\subsection{Architecture}

A meta-dataset is split into two sets, the context set and the target set (also called the query set) where the sets are disjoint $\mathcal{D} = \mathcal{C} \cup \mathcal{T}$ and $\mathcal{C} \cap \mathcal{T} = \emptyset$. The model is trained on the context set and then tested on the target set to see how well it can generalize to new tasks. In essence, our task is to predict the outputs for the target conditioned on the training of the context set.

To achieve this we used a neural network (usually MLPs) to encode the context set into embeddings. 

\begin{equation}
    \bm{r}(\mathcal{C}_i) = \text{Enc}_\theta(\mathcal{C}_i) = \text{Enc}_\theta(\{ \bm{x}_{i}\isup{c}, \bm{y}_i\isup{c} \})
\end{equation}

Where $\mathbf{r_i}$ is the embedding of the context set $\mathcal{C}_i$ and $\theta$ are the parameters of the encoder. Then we aggregate the embeddings of the context sets to get a global representation of the dataset. 

\begin{equation}
    \bm{R(\mathcal{C})} = \text{Agg}(\{ \bm(r)(\mathcal{C}_i) \}_{i=1}^D)
\end{equation}

Typically the aggregation function is a simple summation of the embeddings. The global representation $\bm{R}$ is then used to condition the decoder to predict the outputs of the target set $\mathcal{T} = \{ \bm{x_{i}\isup{t}} \}$ to give us a posterior distribution over the outputs $\bm{y_i\isup{t}}$.

\begin{equation}
    p(\bm{y}_i\isup{t} | \bm{x}_i\isup{t}, \mathcal{C}_i) = \text{Dec}_\theta(\bm{x}_i\isup{t}, \bm{R(\mathcal{C}_i)})
\end{equation}

\subsection{Neural Processes vs Gaussian Processes}

From this it is clear that NPs achieve a \emph{prediction} map, $\pi$ which maps the context set to the target set into a posterior distribution over the outputs. 

\begin{equation}
    \pi: \mathcal{C} \times \mathcal{X}\isup{t} \rightarrow \mathcal{P}(\mathcal{Y})
\end{equation}


This illustrates the similarity to Gaussian Processes (GP) \cite{books/lib/RasmussenW06} which perform the same underlying task of predicting the outputs of a target set conditioned on a context set. The difference is that GPs use a kernel function to compute the covariance between the context and target sets whereas NPs use a neural network to compute the covariance.

\begin{align}
    &\text{GP}: \pi \rightarrow \mathcal{N}(m(
    \mathcal{C}, \bm{x}\isup{t})
    ), k(\bm{x}\isup{c}, \bm{x}\isup{t})) \\
    &\text{NP}: \pi \rightarrow \text{Dec}_\theta(\bm{x}\isup{t},  
    \text{Agg}(\{ \text{Enc}_\theta(\mathcal{C}_i) \}_{i=1}^D))
\end{align}




\subsection{Conditonal Neural Processes}

Conditional Neural Processes (CNPs) \cite{garnelo2018conditional} was one of the two original Neural Processes introduced by Garnelo et al. in 2018. Using the architecture framework described above, CNPs can be described as follows.

\begin{align*}
    \text{Enc}_{\theta_e} = \text{MLP}_{\theta_e} \\
    \text{Agg} = \text{Sum} \\
    \text{Dec}_{\theta_d}  = \text{MLP}_{\theta_d} 
\end{align*}

The encoding and summation is an implementation of the `DeepSet' architecture \cite{zaheer2018deep} which is a neural network that is \textbf{Permutation Invariant} (PI) due to the summation operation. This means that the order of the input does not matter. 


CNPs make the strong assumption that the poster distribution \emph{factorizes} over the target points. This means that the posterior distribution over the target points is independent of each other. 

\eq{
    p(\bm{y}\isup{t} | \bm{x}\isup{t}, \mathcal{C}) = \prod_{i=1}^{N\isup{t}} p(\bm{y}_i\isup{t} | \bm{x}_i\isup{t}, \bm{R}(\mathcal{C}))
}

The benefit of this assumption is that the model can scale linearly with the number of target points with a tractable likelihood. However, this assumption means
\textbf{CNPs are unable to generate coherent sample paths, they are only able to produce distributions over the target points.}

\subsection{Latent Neural Processes}

Neural Processes or better named `Latent Neural Processes' are an extension of CNPs that can generate coherent sample paths. They can do this by using a latent variable $z$ which is created by the encoder. The sampled latent variable is then used by the decoder to generate coherent sample paths.


\ifSubfilesClassLoaded{%
    \printbibliography{}
}{} 


\end{document}